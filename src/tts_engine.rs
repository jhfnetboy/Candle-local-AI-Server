/// TTS Engine - Kokoro-82M ONNX å®ç°
///
/// æ¨¡å‹: Kokoro-82M (82M å‚æ•° TTS æ¨¡å‹)
/// è¿è¡Œæ—¶: ONNX Runtime 2.0-rc
///
/// è¾“å…¥:
/// - tokens: i64 æ•°ç»„ [batch, seq_len]  (éŸ³ç´  token IDs)
/// - style: f32 æ•°ç»„ [1, 256]           (è¯´è¯äººé£æ ¼å‘é‡)
/// - speed: f32 æ•°ç»„ [1]                (è¯­é€Ÿæ§åˆ¶)
///
/// è¾“å‡º:
/// - audio: f32 æ•°ç»„ [batch, audio_len] (24kHz éŸ³é¢‘æ³¢å½¢)

use anyhow::{Context, Result};
use ort::session::{builder::GraphOptimizationLevel, Session};
use std::collections::HashMap;
use std::fs::File;
use std::io::Read;
use std::path::Path;
use std::process::Command;
use tracing::info;

pub struct TTSEngine {
    session: Session,
    sample_rate: u32,
    voices: HashMap<String, Vec<Vec<f32>>>, // voice_name -> [510 tokens, 256 dims]
    default_voice: String,
}

impl TTSEngine {
    /// åˆå§‹åŒ– TTS å¼•æ“
    pub fn new<P: AsRef<Path>>(model_path: P) -> Result<Self> {
        info!("ğŸ”§ TTS å¼•æ“åˆå§‹åŒ–");

        let model_path = model_path.as_ref();
        info!("ğŸ“‚ åŠ è½½æ¨¡å‹: {:?}", model_path);

        // åˆ›å»º ONNX Session
        let session = Session::builder()?
            .with_optimization_level(GraphOptimizationLevel::Level3)?
            .with_intra_threads(4)?
            .commit_from_file(model_path)
            .with_context(|| format!("æ— æ³•åŠ è½½ ONNX æ¨¡å‹: {:?}", model_path))?;

        info!("âœ… ONNX æ¨¡å‹åŠ è½½æˆåŠŸ");

        // æ‰“å°æ¨¡å‹è¾“å…¥/è¾“å‡ºä¿¡æ¯
        info!("ğŸ“‹ ONNX æ¨¡å‹è¾“å…¥:");
        for input in session.inputs.iter() {
            info!("  - åç§°: {}, ç±»å‹: {:?}", input.name, input.input_type);
        }

        info!("ğŸ“‹ ONNX æ¨¡å‹è¾“å‡º:");
        for output in session.outputs.iter() {
            info!("  - åç§°: {}, ç±»å‹: {:?}", output.name, output.output_type);
        }

        // åŠ è½½æ‰€æœ‰ voices
        info!("ğŸ“‚ åŠ è½½æ‰€æœ‰å£°éŸ³...");
        let voices = Self::load_all_voices("data/voices")?;
        info!("âœ… åŠ è½½ {} ä¸ªå£°éŸ³", voices.len());

        let default_voice = "af_alloy".to_string();
        info!("ğŸµ é»˜è®¤å£°éŸ³: {}", default_voice);

        Ok(Self {
            session,
            sample_rate: 24000,
            voices,
            default_voice,
        })
    }

    /// åŠ è½½æ‰€æœ‰å£°éŸ³çš„ style vectors
    fn load_all_voices<P: AsRef<Path>>(voices_dir: P) -> Result<HashMap<String, Vec<Vec<f32>>>> {
        use std::fs;

        let voices_dir = voices_dir.as_ref();
        let index_path = voices_dir.join("index.json");

        // è¯»å–ç´¢å¼•æ–‡ä»¶
        let index_content = fs::read_to_string(&index_path)
            .with_context(|| format!("æ— æ³•è¯»å– index.json: {:?}", index_path))?;

        let index: serde_json::Value = serde_json::from_str(&index_content)?;
        let voices_obj = index.as_object()
            .context("index.json æ ¼å¼é”™è¯¯")?;

        let mut voices = HashMap::new();

        for (voice_name, voice_info) in voices_obj {
            let file_name = voice_info["file"].as_str()
                .context("ç¼ºå°‘ file å­—æ®µ")?;

            let file_path = voices_dir.join(file_name);
            let vectors = Self::load_voice_file(&file_path)?;

            voices.insert(voice_name.clone(), vectors);
        }

        Ok(voices)
    }

    /// åŠ è½½å•ä¸ªå£°éŸ³æ–‡ä»¶
    fn load_voice_file<P: AsRef<Path>>(path: P) -> Result<Vec<Vec<f32>>> {
        let mut file = File::open(path.as_ref())
            .with_context(|| format!("æ— æ³•æ‰“å¼€å£°éŸ³æ–‡ä»¶: {:?}", path.as_ref()))?;

        let mut buffer = Vec::new();
        file.read_to_end(&mut buffer)?;

        // è§£æä¸º f32 æ•°ç»„
        let floats: Vec<f32> = buffer
            .chunks_exact(4)
            .map(|bytes| f32::from_le_bytes([bytes[0], bytes[1], bytes[2], bytes[3]]))
            .collect();

        // é‡ç»„ä¸º [510, 256] ç»“æ„
        const TOKEN_LIMIT: usize = 510;
        const STYLE_DIM: usize = 256;

        let mut vectors = Vec::with_capacity(TOKEN_LIMIT);
        for i in 0..TOKEN_LIMIT {
            let start = i * STYLE_DIM;
            let end = start + STYLE_DIM;
            if end <= floats.len() {
                vectors.push(floats[start..end].to_vec());
            }
        }

        Ok(vectors)
    }

    /// æ–‡æœ¬è½¬è¯­éŸ³ - ONNX æ¨ç†
    pub fn synthesize(&mut self, text: &str, voice: Option<&str>) -> Result<Vec<f32>> {
        let voice_name = voice.unwrap_or(&self.default_voice);
        info!("ğŸµ åˆæˆæ–‡æœ¬: \"{}\" (å£°éŸ³: {})", &text[..text.len().min(50)], voice_name);

        // 1. æ£€æŸ¥æ–‡æœ¬é•¿åº¦ï¼Œå¦‚æœå¤ªé•¿åˆ™åˆ†æ®µå¤„ç†
        const MAX_TOKENS: usize = 400; // å®‰å…¨é™åˆ¶

        // å…ˆè¿›è¡ŒéŸ³ç´ åŒ–ä»¥è·å–å®é™… token æ•°
        let phonemes = self.simple_phonemize(text);
        info!("ğŸ“ éŸ³ç´ : {}", &phonemes[..phonemes.len().min(50)]);

        let tokens = crate::vocab::tokenize(&phonemes);
        info!("ğŸ”¢ Tokens: {} ä¸ª", tokens.len());

        if tokens.is_empty() {
            return Ok(vec![0.0; 24000]); // 1ç§’é™éŸ³
        }

        // å¦‚æœ tokens æ•°è¶…è¿‡é™åˆ¶ï¼ŒæŒ‰å¥å­åˆ†å‰²æ–‡æœ¬é‡æ–°åˆæˆ
        if tokens.len() > MAX_TOKENS {
            info!("âš ï¸ æ–‡æœ¬è¿‡é•¿ ({} tokens > {} é™åˆ¶)ï¼Œè‡ªåŠ¨åˆ†æ®µå¤„ç†", tokens.len(), MAX_TOKENS);
            return self.synthesize_long_text(text, voice);
        }

        // 3. è·å–æŒ‡å®šå£°éŸ³çš„ style vector
        let style_vectors = self.voices.get(voice_name)
            .ok_or_else(|| anyhow::anyhow!("å£°éŸ³ '{}' ä¸å­˜åœ¨", voice_name))?;

        let style_vector = if !style_vectors.is_empty() {
            style_vectors[0].clone()  // ä½¿ç”¨ç¬¬ä¸€ä¸ª token çš„ style
        } else {
            vec![0.0f32; 256]  // é™çº§: é›¶å‘é‡
        };

        info!("ğŸ¨ ä½¿ç”¨å£°éŸ³ '{}' çš„ style vector (dims={})", voice_name, style_vector.len());

        // 4. ONNX æ¨ç†
        let audio = self.run_inference(&tokens, &style_vector)?;

        info!("âœ… ONNX æ¨ç†å®Œæˆ ({} æ ·æœ¬)", audio.len());
        Ok(audio)
    }

    /// åˆ†æ®µåˆæˆé•¿æ–‡æœ¬
    fn synthesize_long_text(&mut self, text: &str, voice: Option<&str>) -> Result<Vec<f32>> {
        // æŒ‰å¥å­åˆ†å‰²ï¼ˆæ”¯æŒ .!? å’Œä¸­æ–‡æ ‡ç‚¹ï¼‰
        let sentences: Vec<&str> = text
            .split(|c: char| c == '.' || c == '!' || c == '?' || c == 'ã€‚' || c == '!' || c == '?')
            .filter(|s| !s.trim().is_empty())
            .collect();

        info!("âœ‚ï¸ æ–‡æœ¬åˆ†å‰²æˆ {} ä¸ªå¥å­", sentences.len());

        let mut combined_audio = Vec::new();
        const SILENCE_SAMPLES: usize = 7200; // 300ms é™éŸ³ (24kHz * 0.3s)
        let silence = vec![0.0f32; SILENCE_SAMPLES];

        for (i, sentence) in sentences.iter().enumerate() {
            let sentence_text = sentence.trim();
            if sentence_text.is_empty() {
                continue;
            }

            info!("ğŸµ åˆæˆç¬¬ {}/{} æ®µ: \"{}\"", i + 1, sentences.len(), &sentence_text[..sentence_text.len().min(50)]);

            // é€’å½’è°ƒç”¨ synthesize (ä¼šå†æ¬¡æ£€æŸ¥é•¿åº¦ï¼Œå¦‚æœå•å¥ä»å¤ªé•¿ä¼šç»§ç»­åˆ†å‰²)
            match self.synthesize(sentence_text, voice) {
                Ok(audio) => {
                    combined_audio.extend_from_slice(&audio);
                    // å¥å­ä¹‹é—´æ·»åŠ çŸ­æš‚é™éŸ³
                    if i < sentences.len() - 1 {
                        combined_audio.extend_from_slice(&silence);
                    }
                }
                Err(e) => {
                    info!("âš ï¸ ç¬¬ {} æ®µåˆæˆå¤±è´¥: {}, è·³è¿‡", i + 1, e);
                    continue;
                }
            }
        }

        if combined_audio.is_empty() {
            return Ok(vec![0.0; 24000]); // è¿”å›1ç§’é™éŸ³
        }

        info!("âœ… é•¿æ–‡æœ¬åˆæˆå®Œæˆ (æ€»æ ·æœ¬æ•°: {})", combined_audio.len());
        Ok(combined_audio)
    }

    /// espeak-ng éŸ³ç´ åŒ–
    fn simple_phonemize(&self, text: &str) -> String {
        match self.phonemize_with_espeak(text) {
            Ok(phonemes) => {
                info!("âœ… espeak-ng éŸ³ç´ åŒ–æˆåŠŸ");
                phonemes
            }
            Err(e) => {
                info!("âš ï¸ espeak-ng å¤±è´¥: {}, ä½¿ç”¨é™çº§æ–¹æ¡ˆ", e);
                // é™çº§: ç®€å•å¤„ç†
                text.chars()
                    .filter(|c| c.is_ascii_alphanumeric() || c.is_whitespace())
                    .collect::<String>()
                    .to_lowercase()
            }
        }
    }

    /// ä½¿ç”¨ espeak-ng è¿›è¡ŒéŸ³ç´ åŒ–
    fn phonemize_with_espeak(&self, text: &str) -> Result<String> {
        info!("ğŸ”Š è°ƒç”¨ espeak-ng: {}", text);
        let output = Command::new("espeak-ng")
            .args(&["-v", "en-us", "-q", "--ipa", text])
            .output()
            .context("espeak-ng æœªå®‰è£…æˆ–æ— æ³•æ‰§è¡Œ")?;

        info!("ğŸ“‹ espeak-ng è¿”å›çŠ¶æ€: {}", output.status);

        if !output.status.success() {
            return Err(anyhow::anyhow!("espeak-ng æ‰§è¡Œå¤±è´¥"));
        }

        let mut phonemes = String::from_utf8(output.stdout)?
            .trim()
            .to_string();

        // Kokoro-specific æ›¿æ¢
        phonemes = phonemes
            .replace("kÉ™kËˆoËÉ¹oÊŠ", "kËˆoÊŠkÉ™É¹oÊŠ")
            .replace("kÉ™kËˆÉ”ËÉ¹É™ÊŠ", "kËˆÉ™ÊŠkÉ™É¹É™ÊŠ")
            .replace("Ê²", "j")
            .replace("r", "É¹")
            .replace("x", "k")
            .replace("É¬", "l");

        // è¿‡æ»¤è¯æ±‡è¡¨å¤–çš„å­—ç¬¦
        phonemes = phonemes
            .chars()
            .filter(|&c| crate::vocab::VOCAB.contains_key(&c))
            .collect();

        Ok(phonemes)
    }

    /// ONNX æ¨ç† (çœŸå®æ•°æ®)
    fn run_inference(&mut self, tokens: &[i64], style_vector: &[f32]) -> Result<Vec<f32>> {
        use ort::value::Tensor;

        // æ·»åŠ  padding tokens (0 = pad token '$')
        let mut padded_tokens = vec![0i64]; // å¼€å§‹ pad
        padded_tokens.extend_from_slice(tokens);
        padded_tokens.push(0); // ç»“æŸ pad

        // åˆ›å»º tokens tensor [1, seq_len]
        let tokens_2d = vec![padded_tokens.clone()];
        let shape = [tokens_2d.len(), tokens_2d[0].len()];
        let tokens_flat: Vec<i64> = tokens_2d.into_iter().flatten().collect();

        info!("ğŸ”¢ Tokenè¾“å…¥: shape={:?}, first_5={:?}", shape, &padded_tokens[..padded_tokens.len().min(5)]);

        let tokens_tensor = Tensor::from_array((shape, tokens_flat))?;

        // åˆ›å»º style tensor [1, 256]
        let style_2d = vec![style_vector.to_vec()];
        let shape_style = [style_2d.len(), style_2d[0].len()];
        let style_flat: Vec<f32> = style_2d.into_iter().flatten().collect();
        let style_tensor = Tensor::from_array((shape_style, style_flat))?;

        // speed: é»˜è®¤é€Ÿåº¦ 1.0
        let speed_tensor = Tensor::from_array(([1], vec![1.0f32]))?;

        info!("ğŸ”§ ONNX è¾“å…¥å‡†å¤‡å®Œæˆ");

        // æ‰§è¡Œæ¨ç† (å‚è€ƒ Kokoros å®ç°)
        let outputs = self.session.run(ort::inputs![
            "input_ids" => tokens_tensor,  // Kokoro v1.0-timestamped ä½¿ç”¨ "input_ids"
            "style" => style_tensor,
            "speed" => speed_tensor,
        ])?;

        info!("âœ… ONNX æ¨ç†æˆåŠŸ");

        // æå–éŸ³é¢‘è¾“å‡º (å°è¯• "waveform" æˆ– "audio")
        let (shape, data) = outputs["waveform"]
            .try_extract_tensor::<f32>()
            .or_else(|_| outputs["audio"].try_extract_tensor::<f32>())
            .context("æ— æ³•æå–éŸ³é¢‘è¾“å‡º")?;

        info!("ğŸµ éŸ³é¢‘å½¢çŠ¶: {:?}", shape);

        // è½¬æ¢ä¸º Vec<f32>
        let mut audio: Vec<f32> = data.to_vec();

        // å½’ä¸€åŒ–éŸ³é¢‘ (é˜²æ­¢å‰Šæ³¢)
        let max_abs = audio.iter()
            .map(|&x| x.abs())
            .fold(0.0f32, |max, x| max.max(x));

        info!("ğŸ“Š éŸ³é¢‘å¹…åº¦èŒƒå›´: max={:.4}", max_abs);

        if max_abs > 1.0 {
            info!("âš ï¸ éŸ³é¢‘å¹…åº¦è¿‡å¤§,è¿›è¡Œå½’ä¸€åŒ–");
            let scale = 0.95 / max_abs; // å½’ä¸€åŒ–åˆ° 95% é¿å…å‰Šæ³¢
            for sample in audio.iter_mut() {
                *sample *= scale;
            }
            info!("âœ… éŸ³é¢‘å·²å½’ä¸€åŒ– (ç¼©æ”¾: {:.4})", scale);
        } else if max_abs > 0.0 {
            // å³ä½¿åœ¨èŒƒå›´å†…,ä¹Ÿæ”¾å¤§åˆ°æ¥è¿‘æœ€å¤§å€¼ä»¥è·å¾—æ›´å¥½çš„éŸ³é‡
            let scale = 0.95 / max_abs;
            if scale > 1.0 {
                for sample in audio.iter_mut() {
                    *sample *= scale;
                }
                info!("ğŸ“ˆ éŸ³é¢‘å¢ç›Š: {:.4}x", scale);
            }
        }

        Ok(audio)
    }

    /// è·å–é‡‡æ ·ç‡
    pub fn sample_rate(&self) -> u32 {
        self.sample_rate
    }
}
