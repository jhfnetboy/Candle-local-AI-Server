/// TTS Engine - Kokoro-82M ONNX å®ç°
///
/// æ¨¡å‹: Kokoro-82M (82M å‚æ•° TTS æ¨¡å‹)
/// è¿è¡Œæ—¶: ONNX Runtime 2.0-rc
///
/// è¾“å…¥:
/// - tokens: i64 æ•°ç»„ [batch, seq_len]  (éŸ³ç´  token IDs)
/// - style: f32 æ•°ç»„ [1, 256]           (è¯´è¯äººé£æ ¼å‘é‡)
/// - speed: f32 æ•°ç»„ [1]                (è¯­é€Ÿæ§åˆ¶)
///
/// è¾“å‡º:
/// - audio: f32 æ•°ç»„ [batch, audio_len] (24kHz éŸ³é¢‘æ³¢å½¢)

use anyhow::{Context, Result};
use ort::session::{builder::GraphOptimizationLevel, Session};
use std::fs::File;
use std::io::Read;
use std::path::Path;
use std::process::Command;
use tracing::info;

pub struct TTSEngine {
    session: Session,
    sample_rate: u32,
    style_vectors: Vec<Vec<f32>>, // [512 tokens, 256 dims]
}

impl TTSEngine {
    /// åˆå§‹åŒ– TTS å¼•æ“
    pub fn new<P: AsRef<Path>>(model_path: P) -> Result<Self> {
        info!("ğŸ”§ TTS å¼•æ“åˆå§‹åŒ–");

        let model_path = model_path.as_ref();
        info!("ğŸ“‚ åŠ è½½æ¨¡å‹: {:?}", model_path);

        // åˆ›å»º ONNX Session
        let session = Session::builder()?
            .with_optimization_level(GraphOptimizationLevel::Level3)?
            .with_intra_threads(4)?
            .commit_from_file(model_path)
            .with_context(|| format!("æ— æ³•åŠ è½½ ONNX æ¨¡å‹: {:?}", model_path))?;

        info!("âœ… ONNX æ¨¡å‹åŠ è½½æˆåŠŸ");

        // æ‰“å°æ¨¡å‹è¾“å…¥/è¾“å‡ºä¿¡æ¯
        info!("ğŸ“‹ ONNX æ¨¡å‹è¾“å…¥:");
        for input in session.inputs.iter() {
            info!("  - åç§°: {}, ç±»å‹: {:?}", input.name, input.input_type);
        }

        info!("ğŸ“‹ ONNX æ¨¡å‹è¾“å‡º:");
        for output in session.outputs.iter() {
            info!("  - åç§°: {}, ç±»å‹: {:?}", output.name, output.output_type);
        }

        // åŠ è½½ style vectors (ä½¿ç”¨ç®€åŒ–çš„äºŒè¿›åˆ¶æ ¼å¼)
        info!("ğŸ“‚ åŠ è½½ style vectors...");
        let style_vectors = Self::load_style_vectors("data/voices_simple.bin")?;
        info!("âœ… åŠ è½½ {} ä¸ª style vectors", style_vectors.len());

        Ok(Self {
            session,
            sample_rate: 24000,
            style_vectors,
        })
    }

    /// åŠ è½½ style vectors from binary file
    /// æ ¼å¼: [510 tokens, 256 dims] f32 æ•°ç»„
    fn load_style_vectors<P: AsRef<Path>>(path: P) -> Result<Vec<Vec<f32>>> {
        let mut file = File::open(path.as_ref())
            .with_context(|| format!("æ— æ³•æ‰“å¼€ style vectors æ–‡ä»¶: {:?}", path.as_ref()))?;

        let mut buffer = Vec::new();
        file.read_to_end(&mut buffer)?;

        // è§£æä¸º f32 æ•°ç»„
        let floats: Vec<f32> = buffer
            .chunks_exact(4)
            .map(|bytes| f32::from_le_bytes([bytes[0], bytes[1], bytes[2], bytes[3]]))
            .collect();

        // é‡ç»„ä¸º [510, 256] ç»“æ„ (Kokoro å®é™…ä½¿ç”¨ 510 ä¸ª token styles)
        const TOKEN_LIMIT: usize = 510;
        const STYLE_DIM: usize = 256;

        let mut vectors = Vec::with_capacity(TOKEN_LIMIT);
        for i in 0..TOKEN_LIMIT {
            let start = i * STYLE_DIM;
            let end = start + STYLE_DIM;
            if end <= floats.len() {
                vectors.push(floats[start..end].to_vec());
            }
        }

        Ok(vectors)
    }

    /// æ–‡æœ¬è½¬è¯­éŸ³ - ONNX æ¨ç†
    pub fn synthesize(&mut self, text: &str) -> Result<Vec<f32>> {
        info!("ğŸµ åˆæˆæ–‡æœ¬: \"{}\"", &text[..text.len().min(50)]);

        // 1. æ–‡æœ¬ â†’ éŸ³ç´  (ç®€åŒ–ç‰ˆ: ç›´æ¥ä½¿ç”¨æ–‡æœ¬)
        let phonemes = self.simple_phonemize(text);
        info!("ğŸ“ éŸ³ç´ : {}", &phonemes[..phonemes.len().min(50)]);

        // 2. éŸ³ç´  â†’ tokens
        let tokens = crate::vocab::tokenize(&phonemes);
        info!("ğŸ”¢ Tokens: {} ä¸ª", tokens.len());

        if tokens.is_empty() {
            return Ok(vec![0.0; 24000]); // 1ç§’é™éŸ³
        }

        // 3. è·å– style vector (ä½¿ç”¨ç¬¬ä¸€ä¸ª style,ä¸ä¾èµ– token é•¿åº¦)
        // å‚è€ƒ Kokoros: ä½¿ç”¨å›ºå®šçš„ voice style
        let style_vector = if !self.style_vectors.is_empty() {
            self.style_vectors[0].clone()  // ä½¿ç”¨ç¬¬ä¸€ä¸ª style (é»˜è®¤å£°éŸ³)
        } else {
            vec![0.0f32; 256]  // é™çº§: é›¶å‘é‡
        };

        info!("ğŸ¨ ä½¿ç”¨ style vector: index=0, dims={}", style_vector.len());

        // 4. ONNX æ¨ç†
        let audio = self.run_inference(&tokens, &style_vector)?;

        info!("âœ… ONNX æ¨ç†å®Œæˆ ({} æ ·æœ¬)", audio.len());
        Ok(audio)
    }

    /// espeak-ng éŸ³ç´ åŒ–
    fn simple_phonemize(&self, text: &str) -> String {
        match self.phonemize_with_espeak(text) {
            Ok(phonemes) => {
                info!("âœ… espeak-ng éŸ³ç´ åŒ–æˆåŠŸ");
                phonemes
            }
            Err(e) => {
                info!("âš ï¸ espeak-ng å¤±è´¥: {}, ä½¿ç”¨é™çº§æ–¹æ¡ˆ", e);
                // é™çº§: ç®€å•å¤„ç†
                text.chars()
                    .filter(|c| c.is_ascii_alphanumeric() || c.is_whitespace())
                    .collect::<String>()
                    .to_lowercase()
            }
        }
    }

    /// ä½¿ç”¨ espeak-ng è¿›è¡ŒéŸ³ç´ åŒ–
    fn phonemize_with_espeak(&self, text: &str) -> Result<String> {
        info!("ğŸ”Š è°ƒç”¨ espeak-ng: {}", text);
        let output = Command::new("espeak-ng")
            .args(&["-v", "en-us", "-q", "--ipa", text])
            .output()
            .context("espeak-ng æœªå®‰è£…æˆ–æ— æ³•æ‰§è¡Œ")?;

        info!("ğŸ“‹ espeak-ng è¿”å›çŠ¶æ€: {}", output.status);

        if !output.status.success() {
            return Err(anyhow::anyhow!("espeak-ng æ‰§è¡Œå¤±è´¥"));
        }

        let mut phonemes = String::from_utf8(output.stdout)?
            .trim()
            .to_string();

        // Kokoro-specific æ›¿æ¢
        phonemes = phonemes
            .replace("kÉ™kËˆoËÉ¹oÊŠ", "kËˆoÊŠkÉ™É¹oÊŠ")
            .replace("kÉ™kËˆÉ”ËÉ¹É™ÊŠ", "kËˆÉ™ÊŠkÉ™É¹É™ÊŠ")
            .replace("Ê²", "j")
            .replace("r", "É¹")
            .replace("x", "k")
            .replace("É¬", "l");

        // è¿‡æ»¤è¯æ±‡è¡¨å¤–çš„å­—ç¬¦
        phonemes = phonemes
            .chars()
            .filter(|&c| crate::vocab::VOCAB.contains_key(&c))
            .collect();

        Ok(phonemes)
    }

    /// ONNX æ¨ç† (çœŸå®æ•°æ®)
    fn run_inference(&mut self, tokens: &[i64], style_vector: &[f32]) -> Result<Vec<f32>> {
        use ort::value::Tensor;

        // æ·»åŠ  padding tokens (0 = pad token '$')
        let mut padded_tokens = vec![0i64]; // å¼€å§‹ pad
        padded_tokens.extend_from_slice(tokens);
        padded_tokens.push(0); // ç»“æŸ pad

        // åˆ›å»º tokens tensor [1, seq_len]
        let tokens_2d = vec![padded_tokens.clone()];
        let shape = [tokens_2d.len(), tokens_2d[0].len()];
        let tokens_flat: Vec<i64> = tokens_2d.into_iter().flatten().collect();

        info!("ğŸ”¢ Tokenè¾“å…¥: shape={:?}, first_5={:?}", shape, &padded_tokens[..padded_tokens.len().min(5)]);

        let tokens_tensor = Tensor::from_array((shape, tokens_flat))?;

        // åˆ›å»º style tensor [1, 256]
        let style_2d = vec![style_vector.to_vec()];
        let shape_style = [style_2d.len(), style_2d[0].len()];
        let style_flat: Vec<f32> = style_2d.into_iter().flatten().collect();
        let style_tensor = Tensor::from_array((shape_style, style_flat))?;

        // speed: é»˜è®¤é€Ÿåº¦ 1.0
        let speed_tensor = Tensor::from_array(([1], vec![1.0f32]))?;

        info!("ğŸ”§ ONNX è¾“å…¥å‡†å¤‡å®Œæˆ");

        // æ‰§è¡Œæ¨ç† (å‚è€ƒ Kokoros å®ç°)
        let outputs = self.session.run(ort::inputs![
            "input_ids" => tokens_tensor,  // Kokoro v1.0-timestamped ä½¿ç”¨ "input_ids"
            "style" => style_tensor,
            "speed" => speed_tensor,
        ])?;

        info!("âœ… ONNX æ¨ç†æˆåŠŸ");

        // æå–éŸ³é¢‘è¾“å‡º (å°è¯• "waveform" æˆ– "audio")
        let (shape, data) = outputs["waveform"]
            .try_extract_tensor::<f32>()
            .or_else(|_| outputs["audio"].try_extract_tensor::<f32>())
            .context("æ— æ³•æå–éŸ³é¢‘è¾“å‡º")?;

        info!("ğŸµ éŸ³é¢‘å½¢çŠ¶: {:?}", shape);

        // è½¬æ¢ä¸º Vec<f32>
        let mut audio: Vec<f32> = data.to_vec();

        // å½’ä¸€åŒ–éŸ³é¢‘ (é˜²æ­¢å‰Šæ³¢)
        let max_abs = audio.iter()
            .map(|&x| x.abs())
            .fold(0.0f32, |max, x| max.max(x));

        info!("ğŸ“Š éŸ³é¢‘å¹…åº¦èŒƒå›´: max={:.4}", max_abs);

        if max_abs > 1.0 {
            info!("âš ï¸ éŸ³é¢‘å¹…åº¦è¿‡å¤§,è¿›è¡Œå½’ä¸€åŒ–");
            let scale = 0.95 / max_abs; // å½’ä¸€åŒ–åˆ° 95% é¿å…å‰Šæ³¢
            for sample in audio.iter_mut() {
                *sample *= scale;
            }
            info!("âœ… éŸ³é¢‘å·²å½’ä¸€åŒ– (ç¼©æ”¾: {:.4})", scale);
        } else if max_abs > 0.0 {
            // å³ä½¿åœ¨èŒƒå›´å†…,ä¹Ÿæ”¾å¤§åˆ°æ¥è¿‘æœ€å¤§å€¼ä»¥è·å¾—æ›´å¥½çš„éŸ³é‡
            let scale = 0.95 / max_abs;
            if scale > 1.0 {
                for sample in audio.iter_mut() {
                    *sample *= scale;
                }
                info!("ğŸ“ˆ éŸ³é¢‘å¢ç›Š: {:.4}x", scale);
            }
        }

        Ok(audio)
    }

    /// è·å–é‡‡æ ·ç‡
    pub fn sample_rate(&self) -> u32 {
        self.sample_rate
    }
}
